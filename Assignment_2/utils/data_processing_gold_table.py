# data_process_gold_tables.py
import os, sys, yaml, logging
from datetime import datetime
from dateutil.relativedelta import relativedelta
import pyspark.sql.functions as F
from pyspark.sql import SparkSession

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# -------------------------------------------------------------
# Helpers
# -------------------------------------------------------------
def resolve_relative_path(path):
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_dir = os.path.dirname(script_dir)
    return os.path.abspath(os.path.join(base_dir, path))

def aggregate_clickstream(clicks, feature_cols, loans):
    """Aggregate clickstream 3-month history (avg/sum) and last-month snapshot."""
    c, l = clicks.alias("c"), loans.select("Customer_ID", "loan_start_date").alias("l")
    joined = c.join(l, "Customer_ID")

    l3m = (
        joined
        .where(
            (F.col("c.snapshot_date") >= F.add_months(F.col("l.loan_start_date"), -3)) &
            (F.col("c.snapshot_date") <  F.col("l.loan_start_date"))
        )
        .groupBy("c.Customer_ID", "l.loan_start_date")
        .agg(
            *[F.avg(F.col(f"c.{f}")).alias(f"avg_{f}_L3M") for f in feature_cols],
            *[F.sum(F.col(f"c.{f}")).alias(f"sum_{f}_L3M") for f in feature_cols]
        )
    )

    lm = (
        joined
        .where(F.col("c.snapshot_date") == F.add_months(F.col("l.loan_start_date"), -1))
        .groupBy("c.Customer_ID", "l.loan_start_date")
        .agg(*[F.first(F.col(f"c.{f}")).alias(f"{f}_LM") for f in feature_cols])
    )

    return l3m.join(lm, ["Customer_ID", "loan_start_date"], "left")

# -------------------------------------------------------------
# Main
# -------------------------------------------------------------
def process_gold_tables(snapshot_date, spark, config_path="config/gold_config.yaml"):
    if not snapshot_date:
        raise ValueError("snapshot_date (YYYY-MM-DD) required.")
    snapshot_dt = datetime.strptime(snapshot_date, "%Y-%m-%d")

    # --- Load config ---
    cfg_path = resolve_relative_path(config_path)
    with open(cfg_path) as f:
        cfg = yaml.safe_load(f)

    silver_base = resolve_relative_path(cfg["paths"]["silver_base"])
    gold_base   = resolve_relative_path(cfg["paths"]["gold_base"])
    static_cfg  = cfg["static_features"]
    click_cols  = cfg["clickstream_features"]
    loan_cfg    = cfg["loan_types"]

    logger.info(f"ðŸš€ Building Gold tables for {snapshot_date}")

    # ---------------------------------------------------------
    # Load current silver tables
    # ---------------------------------------------------------
    tag = snapshot_date.replace("-", "_")
    attrs = spark.read.parquet(os.path.join(silver_base, "features_attributes", f"{tag}.parquet"))
    fins  = spark.read.parquet(os.path.join(silver_base, "features_financials", f"{tag}.parquet"))
    loans = spark.read.parquet(os.path.join(silver_base, "lms_loan_daily", f"{tag}.parquet"))

    # ---------------------------------------------------------
    # Load last 3 months of clickstream for L3M aggregation
    # ---------------------------------------------------------
    click_dfs = []
    missing_months = []
    for i in range(3, 0, -1):
        prev_tag = (snapshot_dt - relativedelta(months=i)).strftime("%Y_%m_%d")
        path = os.path.join(silver_base, "feature_clickstream", f"{prev_tag}.parquet")
        if os.path.exists(path):
            click_dfs.append(spark.read.parquet(path))
        else:
            missing_months.append(prev_tag)
    
    if missing_months:
        logger.warning(f"âš ï¸ Missing clickstream months: {missing_months}")
    
    # Require full 3 months of history
    if len(click_dfs) < 3:
        logger.info(f"â­ï¸ Skipping feature_store generation for {snapshot_date}: insufficient L3M clickstream data.")
        return None    

    # If L3M data is available
    clicks = click_dfs[0]
    for df in click_dfs[1:]:
        clicks = clicks.unionByName(df, allowMissingColumns=True)

    # ---------------------------------------------------------
    # 1ï¸âƒ£  Feature store  (for current snapshot loans)
    # ---------------------------------------------------------
    attr_cols = static_cfg["attributes"]
    fin_cols  = static_cfg["financials"]

    static_feats = (
        attrs.alias("a")
             .join(fins.alias("f"), ["Customer_ID", "snapshot_date"], "inner")
             .select("Customer_ID", "snapshot_date", *attr_cols, *fin_cols)
    )

    feature_store = (
        loans.select("Customer_ID", "loan_start_date")
             .join(
                 static_feats,
                 (static_feats.Customer_ID == loans.Customer_ID) &
                 (static_feats.snapshot_date == loans.loan_start_date),
                 "left"
             )
             .drop(static_feats.Customer_ID, static_feats.snapshot_date)
    )

    if clicks is not None:
        click_aggs = aggregate_clickstream(clicks, click_cols, loans)
        feature_store = feature_store.join(click_aggs, ["Customer_ID", "loan_start_date"], "left")
        click_cols_expanded = [c for c in feature_store.columns if c.endswith("_L3M") or c.endswith("_LM")]
        feature_store = feature_store.fillna(0, subset=click_cols_expanded)

    # Multi-hot encode loan types
    expected_loans = loan_cfg["expected"]
    default_loan   = loan_cfg["default"]
    feature_store = feature_store.withColumn(
        "Type_of_Loan_list",
        F.when(F.size(F.col("Type_of_Loan_list")) == 0, F.array(F.lit(default_loan)))
         .when(F.col("Type_of_Loan_list").isNull(), F.array(F.lit(default_loan)))
         .otherwise(F.col("Type_of_Loan_list"))
    )
    for loan in expected_loans + [default_loan]:
        feature_store = feature_store.withColumn(
            f"LoanType_{loan.replace(' ', '_')}",
            F.when(F.array_contains(F.col("Type_of_Loan_list"), loan), 1).otherwise(0)
        )
    feature_store = feature_store.drop("Type_of_Loan_list")

    # ---------------------------------------------------------
    # 2ï¸âƒ£  Label store  (loans whose 4 MOBs completed)
    # ---------------------------------------------------------
    mob_window = 4
    label_store = (
        loans.filter(F.col("mob") <= mob_window)
             .groupBy("Customer_ID", "loan_start_date")
             .agg(F.max(F.col("dpd_flag")).alias("label"))
             .withColumn("label_def", F.lit(f"30dpd_{mob_window}mob"))
             .filter(F.add_months(F.col("loan_start_date"), mob_window) <= F.lit(snapshot_dt))
             .withColumn("snapshot_date", F.lit(snapshot_dt))
    )

    # ---------------------------------------------------------
    # Save outputs
    # ---------------------------------------------------------
    feat_out = os.path.join(gold_base, "feature_store", f"{tag}.parquet")
    lab_out  = os.path.join(gold_base, "label_store",   f"{tag}.parquet")
    os.makedirs(os.path.dirname(feat_out), exist_ok=True)
    os.makedirs(os.path.dirname(lab_out),  exist_ok=True)

    feature_store.coalesce(1).write.mode("overwrite").parquet(feat_out)
    label_store.coalesce(1).write.mode("overwrite").parquet(lab_out)

    logger.info(f"âœ… Feature store saved: {feat_out}")
    logger.info(f"âœ… Label store saved:   {lab_out}")
    logger.info(f"ðŸŽ‰ Completed Gold ETL for {snapshot_date}")
    return True

# -------------------------------------------------------------
# Entrypoint
# -------------------------------------------------------------
if __name__ == "__main__":
    spark = SparkSession.builder.appName("GoldLayerProcessing").getOrCreate()
    snapshot_date = sys.argv[1] if len(sys.argv) > 1 else None
    config_path   = sys.argv[2] if len(sys.argv) > 2 else "config/gold_config.yaml"
    process_gold_tables(snapshot_date, spark, config_path)
    spark.stop()
